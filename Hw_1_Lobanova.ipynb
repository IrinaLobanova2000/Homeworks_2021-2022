{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw_1_Lobanova",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZhVujzO281c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8adae93-0432-47e9-d41e-06adb28df321"
      },
      "source": [
        "!pip install python-rake"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-rake\n",
            "  Downloading python_rake-1.5.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: python-rake\n",
            "Successfully installed python-rake-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlNJ-cprW6Ox",
        "outputId": "9ec53c71-731d-4915-8fdd-4549da73da05"
      },
      "source": [
        "!pip3 install summa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=c2638aa36cdd138e744752df64a49b93896dfded95767706c5d9e6b1301a6d80\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ5muaj3rK3d",
        "outputId": "5fd37ec3-fc0c-4e79-c5d2-fad1bd2c1dfd"
      },
      "source": [
        "!pip install rank_bm25\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.1-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDVL20eDXSD0",
        "outputId": "b46e27c6-482a-4c32-bfed-e68dcb79bd49"
      },
      "source": [
        "!pip3 install pymorphy2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 36.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtmgmbuaSh17",
        "outputId": "7417dce6-85e7-455d-9603-85b88af64c68"
      },
      "source": [
        "import RAKE\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndjBV5qg6k0M"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4ZxGyqW_GC"
      },
      "source": [
        "from summa import keywords"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdP3fqNKXQH_"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "from pymorphy2.tokenizers import simple_word_tokenize"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKRs990MrRvl"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.summarization.bm25 import get_bm25_weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLOQRrDSwSZ"
      },
      "source": [
        "stop = stopwords.words('russian')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFvK2sT0S5BP"
      },
      "source": [
        "Тексты были взяты с сайта газеты \"Известия\" (ключевые слова располагаются вверху, над текстом статьи):\n",
        "https://iz.ru/1241783/evgeniia-priemskaia/dar-rechi-kto-sokhraniaet-iazyki-korennykh-narodov\n",
        "https://iz.ru/1241275/sergei-sychev/vse-bezhish-forrest\n",
        "https://iz.ru/1244125/aleksei-zabrodin/liubiat-ne-liubiat\n",
        "https://iz.ru/1239085/dmitrii-orlov/sokhranit-vs-izmeneniia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrzPiPp7YGqd"
      },
      "source": [
        "Отобранные ключевые слова (я объединила со своими и убрала некоторые, которые были на сайте)\n",
        "Изначально были такие (на сайте):\n",
        "1.национальные языки, народы, сибирь, дальний восток\n",
        "2.кино, кинопремьеры\n",
        "3.джо байден, сша, рейтинги\n",
        "4.владимир путин, валдайский клуб, реджеп эрдоган, джо байден\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHaGz8-FYFo7"
      },
      "source": [
        "key_words = ['национальные языки, народы, сибирь, дальний восток, коренные малочисленные народы, коренные народы, родной язык, проект \"энцы\", хабаровский край, язык, народы, эвенкийский язык',\n",
        "'фильм, Финч, Том Хэнкс, робот',             \n",
        "'джо байден, сша, рейтинг, политика, белый дом, президент, демократия, демократический',\n",
        "'владимир путин, страна, политика, государство, клуб Валдай, 2021 год, Россия'             \n",
        "]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBUygjn_S22h"
      },
      "source": [
        "with open('text1.txt', 'r', encoding='utf-8') as f:\n",
        "  text1 = f.read()\n",
        "with open('text2.txt', 'r', encoding='utf-8') as f:\n",
        "  text2 = f.read()\n",
        "with open('text3.txt', 'r', encoding='utf-8') as f:\n",
        "  text3 = f.read()\n",
        "with open('text4.txt', 'r', encoding='utf-8') as f:\n",
        "  text4 = f.read()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcCwTs1TXYXH"
      },
      "source": [
        "m = MorphAnalyzer()\n",
        "def normalize_text(text1):\n",
        "    lemmas = []\n",
        "    for t in simple_word_tokenize(text1):\n",
        "        lemmas.append(\n",
        "            m.parse(t)[0].normal_form\n",
        "        )\n",
        "    return ' '.join(lemmas)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwQA_rnzcYoW",
        "outputId": "f7393ee4-d8d0-4fb2-d3cb-103ffe9dd0d2"
      },
      "source": [
        "norm_key_words = []\n",
        "for i in key_words:\n",
        "  num = []\n",
        "  i = normalize_text(i)\n",
        "  num.append (i)\n",
        "  norm_key_words.append(num)\n",
        "print(norm_key_words)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['национальный язык , народ , сибирь , дальний восток , коренной малочисленный народ , коренной народ , родный язык , проект \" энца \" , хабаровский край , язык , народ , эвенкийский язык'], ['фильм , финч , тот хэнкс , робот'], ['джо байден , сша , рейтинг , политика , белый дом , президент , демократия , демократический'], ['владимир путин , страна , политика , государство , клуб валдай , 2021 год , россия']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7HgeVGKTdzQ",
        "outputId": "a5c95c45-34f8-4b5f-b9aa-d09ce5fca6e2"
      },
      "source": [
        "def raking(text):\n",
        "  stop.append('который')\n",
        "  rake = RAKE.Rake(stop)\n",
        "  text_list = rake.run(normalize_text(text), maxWords=2, minFrequency=2)\n",
        "  return text_list\n",
        "rake1 = raking(text1)[:12]\n",
        "rake2 =raking(text2)[:4]\n",
        "rake3 =raking(text3)[:8]\n",
        "rake4 =raking(text4)[:7]\n",
        "print(rake1)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('хабаровский край', 4.0), ('учебный пособие', 4.0), ('дальний восток', 4.0), ('рабочий тетрадь', 4.0), ('эвенкийский язык', 3.6875), ('это заниматься', 3.666666666666667), ('изучение язык', 3.520833333333333), ('язык', 1.6875), ('это', 1.6666666666666667), ('учебник', 1.5454545454545454), ('проживать', 1.3333333333333333), ('численность', 1.3333333333333333)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPSCfkzYXAiz"
      },
      "source": [
        "key1 = keywords.keywords(normalize_text(text1), language='russian', additional_stopwords=stop, scores=True)[:12]\n",
        "key2 = keywords.keywords(normalize_text(text2), language='russian', additional_stopwords=stop, scores=True)[:4]\n",
        "key3 = keywords.keywords(normalize_text(text3), language='russian', additional_stopwords=stop, scores=True)[:8]\n",
        "key4 = keywords.keywords(normalize_text(text4), language='russian', additional_stopwords=stop, scores=True)[:7]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfYq3wYNW1em",
        "outputId": "6e302a47-d771-4cfb-89f3-0b6d51b36c6b"
      },
      "source": [
        "!pip install rutermextract"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rutermextract\n",
            "  Downloading rutermextract-0.3.tar.gz (8.1 kB)\n",
            "Requirement already satisfied: pymorphy2>=0.8 in /usr/local/lib/python3.7/dist-packages (from rutermextract) (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.8->rutermextract) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.8->rutermextract) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.8->rutermextract) (2.4.417127.4579844)\n",
            "Building wheels for collected packages: rutermextract\n",
            "  Building wheel for rutermextract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rutermextract: filename=rutermextract-0.3-py3-none-any.whl size=9722 sha256=b7691ff4c70cc9c593bbe061154fbbf74fd8db6fb99ae1cba9ebe30f2afb3010\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/0a/8d/810c38b53b6bfe00a9d964bba36077c633a717e5240883ccbf\n",
            "Successfully built rutermextract\n",
            "Installing collected packages: rutermextract\n",
            "Successfully installed rutermextract-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66K8Ev8rVRW"
      },
      "source": [
        "from rutermextract import TermExtractor\n",
        "def rutermextract(text):\n",
        "  term_extractor = TermExtractor()\n",
        "  key_terms = []\n",
        "  for term in term_extractor(text):\n",
        "    termin = []\n",
        "    termin.append(term.normalized)\n",
        "    termin.append(term.count)\n",
        "    key_terms.append(termin)\n",
        "  return key_terms[:20]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASJPFBmVXb61"
      },
      "source": [
        "rut1 = rutermextract(normalize_text(text1))[:12]\n",
        "rut2 = rutermextract(normalize_text(text2))[:4]\n",
        "rut3 = rutermextract(normalize_text(text3))[:8]\n",
        "rut4 = rutermextract(normalize_text(text4))[:7]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUzm6GYwQcQT"
      },
      "source": [
        "samples = [['ADJF', 'NOUN'],['NOUN', 'ADJF'],['NOUN', 'NOUN', 'NOUN'],['NOUN', 'NOUN'],['NOUN']]\n",
        "def right_samples(key_words, samples): \n",
        "    filtered = [] \n",
        "    for i in key_words: \n",
        "        right_samples = [] \n",
        "        m = MorphAnalyzer() \n",
        "        p = m.parse(i)[0] \n",
        "        pos = p.tag.POS \n",
        "        right_samples.append(pos) \n",
        "        for sample in samples: \n",
        "            if right_samples == sample: \n",
        "                filtered.append(i) \n",
        "    return filtered\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgSt17kf5K7X",
        "outputId": "7c8c5af0-4287-4888-887d-f1e4940c639e"
      },
      "source": [
        "list_of_samples = right_samples(norm_key_words[0], samples)\n",
        "print(list_of_samples)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['национальный язык , народ , сибирь , дальний восток , коренной малочисленный народ , коренной народ , родный язык , проект \" энца \" , хабаровский край , язык , народ , эвенкийский язык']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hqhO3pn8wYg",
        "outputId": "bda9c67c-9990-472a-862e-5b2b069a0c0a"
      },
      "source": [
        "def score(my_keywords, result):\n",
        "    print(my_keywords[0])\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    for i, tokens in enumerate(result):\n",
        "        n = 0\n",
        "        for token in tokens:\n",
        "            token = str(token).split(',')\n",
        "            if str(token[0]) in str(my_keywords[0]):\n",
        "                n += 1\n",
        "        recall = n / len(my_keywords)\n",
        "        precision = n / len(tokens)\n",
        "\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "    return precision, recall\n",
        "    \n",
        "score(norm_key_words[0],key1)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "национальный язык , народ , сибирь , дальний восток , коренной малочисленный народ , коренной народ , родный язык , проект \" энца \" , хабаровский край , язык , народ , эвенкийский язык\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    }
  ]
}